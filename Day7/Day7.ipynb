{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                       Workshop 7: Maria Balaet and Valentina Giunchiglia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on drug use free text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this workshop is to introduce you the application of **natural language processing (NLP)** algorithms to study free text data. NLP is a branch of artificial intelligence that focuses on trying to understand written and spoken text. In this workshop, we will focus specifically on **topic modelling**, which is an unsupervised machine learning method that automatically analyses groups of words to identify clusters of words that belong to the same topic, or theme.\n",
    "\n",
    "In particular, we will use topic. modelling to try to understand the reasons behind the change in drug use patterns of recreational drug users during the early stages of the pandemic. In the morning, we will investigate why recreational drug users decided to increase their use, later in the day you will apply what you learnt in this stage of the workshop to understand why they decided to decrease their drug use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download and import the packages we will need during the lecture, and to change the display settings in order to be able to visualise more rows and columns when printing dataframes. Today, we will work a lot with two new python modules called `gensim` and `nltk`. `gensim` is one of the most coimmonly used module for topic modelling in Python, and `nltk` is a NLP Python toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings \n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the data that we will use during the workshop and let's check them out. As you can see, the dataframe consists of three columns that report the user ids, whether the drug use of participants increased or decreased during the pandemic and the reasons behind this change in format of free text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more = pd.read_csv(\"Data/Day7_morning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>How has your drug use changed due to the pandemic?</th>\n",
       "      <th>Why has your drug use changed during the pandemic?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107cee6d-0024-4861-90be-2f467758776c</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Increase in marijuana use for relaxation/stress relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2feae3b8-ea3b-4750-acec-02418f995199</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Microdosing psilocybin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a476ba2-6dbb-42d1-b0f5-c877c0f368d8</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3f8b0ac6-200f-49b4-b061-70084536d519</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4be44530-aeab-4302-bd92-b000b4eae2fb</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Went to university</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>6fbf9bf3-1e5a-4fde-98be-6015accde42b</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>e262433c-094a-4f35-8b6a-0c6689b2185f</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>8862877f-c12e-4680-8b7f-04657ae97cbd</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>More time at home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>035a1239-932f-4dc2-9ae8-4ccad6e7be08</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>I'm more bored and frightened when I read the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>b277b233-b4b3-4fe0-9b5c-ec6cd26a67d8</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Spending more time at home.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id  \\\n",
       "0    107cee6d-0024-4861-90be-2f467758776c   \n",
       "1    2feae3b8-ea3b-4750-acec-02418f995199   \n",
       "2    3a476ba2-6dbb-42d1-b0f5-c877c0f368d8   \n",
       "3    3f8b0ac6-200f-49b4-b061-70084536d519   \n",
       "4    4be44530-aeab-4302-bd92-b000b4eae2fb   \n",
       "..                                    ...   \n",
       "285  6fbf9bf3-1e5a-4fde-98be-6015accde42b   \n",
       "286  e262433c-094a-4f35-8b6a-0c6689b2185f   \n",
       "287  8862877f-c12e-4680-8b7f-04657ae97cbd   \n",
       "288  035a1239-932f-4dc2-9ae8-4ccad6e7be08   \n",
       "289  b277b233-b4b3-4fe0-9b5c-ec6cd26a67d8   \n",
       "\n",
       "    How has your drug use changed due to the pandemic?  \\\n",
       "0                                      I am using more   \n",
       "1                                      I am using more   \n",
       "2                                      I am using more   \n",
       "3                                      I am using more   \n",
       "4                                      I am using more   \n",
       "..                                                 ...   \n",
       "285                                    I am using more   \n",
       "286                                    I am using more   \n",
       "287                                    I am using more   \n",
       "288                                    I am using more   \n",
       "289                                    I am using more   \n",
       "\n",
       "                                                                         Why has your drug use changed during the pandemic?  \n",
       "0                                                                    Increase in marijuana use for relaxation/stress relief  \n",
       "1                                                                                                   Microdosing psilocybin   \n",
       "2                                                                                                                   Boredom  \n",
       "3    I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic  \n",
       "4                                                                                                       Went to university   \n",
       "..                                                                                                                      ...  \n",
       "285                                                                                                                 boredom  \n",
       "286                                                                                                                Boredom   \n",
       "287                                                                                                       More time at home  \n",
       "288                                                                      I'm more bored and frightened when I read the news  \n",
       "289                                                                                            Spending more time at home.   \n",
       "\n",
       "[290 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasons_for_change_dec2020_more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------\n",
    "### Code here\n",
    "The column names of the dataframes are quite long and the row names are not really easy to interpret. \n",
    "\n",
    "1. Replace the column headers with the names \"how\" and \"why\" respectively for \"How has your drug use changed due to the pandemic?\" and \"Why has your drug use changed during the pandemic?\". \n",
    "2. Set the user ids as row index\n",
    "3. Since we are analysing only the participants, the \"how\" column should have only \"I am using more\" answers, check that it is the case.\n",
    "4. Confirm that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE\n",
    "reasons_for_change_dec2020_more.rename(columns = {\"Why has your drug use changed during the pandemic?\": \"why\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "Now that we have a better looking dataframe, let's check out the answers in the *why* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By quickly looking at the answers, it appears that one of the main reasons for starting to use more drugs during the pandemic was *boredom*. However, different people express the same concept in slightly different ways. Let's print a few answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic \n",
      " Boredom/accessibility/situational. I have done E twice during this period, separated by around 3 months, and ket twice, separated by about 4 months, so I'm not concerned about restricting myself, or doing so to excess. \n",
      " Boredom and loneliness \n"
     ]
    }
   ],
   "source": [
    "print(reasons_for_change_dec2020_more['why'][3], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][14],\"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][34]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look more carefully, you will notice a few other things: 1) some people wrote boredom with or without capital letters, 2) some answers have empty spaces, 3) other answers have special characters, 4) some words are spelled incorrectly... All these aspects are just few examples of the noise that free text answers have, and that need to be removed before starting any analysis. The data cleaning step in free text analysis is **fundamental**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boredom \n",
      " Boredom  \n",
      " / \n",
      " Boredom but stopped.now \n",
      " Bordem \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reasons_for_change_dec2020_more['why'][285], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][286],\"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][281], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][278], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][273], \"\\n\",\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data cleaning is necessary to remove errors in the data, and reduce to the minimum the noise in order to include in the analysis only what is essential. The most important data cleaning steps of free text data are the following:\n",
    "\n",
    "1. **Turning all letters to lower case:** this is important otherwise words with capital letters will be mistakenly be recognised as different compared to the same words without capital letters (e.g This and this).\n",
    "2. **Removal of punctuation, special characters and digits**: punctuation creates noise in the data. It cannot be used to make sense of the meaning of a topic because it does not represent words and computer don't know how to interpret it.\n",
    "3. **Tokenization**: method that consists of separating a piece of text (in this case the answers of each participants) into smaller linguistic units called tokens (in this case words). \n",
    "4. **Stop words removal**: method that consists in removing words that are really common in english, but don't provide much information, such as \"to\", \"in\" or \"when. These words just create noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete these cleaning steps. First, we will turn everything to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove all punctuations (e.g. *,* or *.*), special characters (e.g. *?,/* and *&*) and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.replace('[,\\.!?/&]', '')\n",
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the free text data into a separate variable, and let's check it out. Were all digits, punctuations, and special characters removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reasons_for_change_dec2020_more['why'].to_list()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of the data cleaning and preparation is **tokenization**. Tokenization is necessary to make the sentences analysable and understandable for the computer, and consists of splitting each answer of participants into lists of individual words. There is a function in the gensim package that can do this directly, by taking as input each separate answer, called `simple_preprocess`. By providing as argument `deacc=True`, the function removes punctuations if it finds any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fewer', 'events', 'makes', 'for', 'more', 'exciting', 'evening']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = []\n",
    "for sentence in data:\n",
    "    listwords = gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "    data_words.append(listwords)\n",
    "    \n",
    "data_words[10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different elements in the *data_words* list. Do yo understand how tokenization works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the answers in terms of lists of words, we can do some cleaning on the words themselves. The first thing we are going to do is to remove the stop words, or commonly used words in the english language. Luckily, the *nltk* module has alredy a list of these words that we can simply download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valentinagiunchiglia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = nltk.corpus.stopwords.words('english') \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you think that other words are too common and should be removed, but they are not in this list, you can easily add them. Adding extra filter is always a good idea. In this way, the noise in the data is reduced even more. Do you have any other words in mind? If you do, add it to the following list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'goes', 'with']\n"
     ]
    }
   ],
   "source": [
    "custom_stop = ['goes','with'] \n",
    "finalstop = stop + custom_stop\n",
    "print(finalstop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final list of stop words, we can remove those words from the list of words of each participants' answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_final = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/emsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_all] for doc in texts]\n",
    "\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "#LEMMATIZATION. this is perhaps the most important step. to reduce the dimensionality of data, lematization turns each word to its dictionary infinitive form.\n",
    "#so words like \"swim\", \"swam\", and \"swum\" would all be converted to \"swim\"\n",
    "lemma = WordNetLemmatizer() #import the lemmatizer function\n",
    "\n",
    "lematized = [[lemma.lemmatize(word) for word in data_words_list] for data_words_list in data_words]\n",
    "\n",
    "#remove empty lists\n",
    "\n",
    "complete = [x for x in lematized  if x]\n",
    "\n",
    "reasons_for_change_dec2020_more_cleaned=complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify optimal topics using LDA\n",
    "\n",
    "\n",
    "\n",
    "#coherence of topics with no held out set\n",
    "#Construct df\n",
    "\n",
    "from time import time\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tmtoolkit\n",
    "import numpy\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "\n",
    "# Topics range - defining the number of topics that we are going to trial to see which one would work as the optimal split\n",
    "min_topics = 1\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "#create the dataframe where the results will be stored\n",
    "results = {'Topics': [], \"cv_Coherence_avg\": []}\n",
    "    \n",
    "       \n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(reasons_for_change_dec2020_more_cleaned)\n",
    "\n",
    "# Create Corpus\n",
    "texts = reasons_for_change_dec2020_more_cleaned\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "for i in topics_range:\n",
    "    n_topics = i\n",
    "\n",
    "    #this is the LDA function. this will act as a multithreaded engine that will split the data into a given number of topics. \n",
    "    #The word ‘Latent’ indicates that the model discovers the ‘yet-to-be-found’ or hidden topics from the documents. \n",
    "    #‘Dirichlet’ indicates LDA’s two assumptions of LDA - that both the distribution of topics withinin a document \n",
    "    #and the distribution of words within each topic are Dirichlet distributions. \n",
    "    #‘Allocation’ indicates the distribution of topics in the document.\n",
    "\n",
    "    #LDA assumes that the words within a document can be used determine the topics. \n",
    "    #LDA assigns each word in a document to different topic, then maps the entire document to a list of topics.\n",
    "    #Put another way, LDA computes a many-to-many relationship between topics and words, and thus a many-to-many relationship between documents and topics.\n",
    "    \n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=n_topics, \n",
    "                                                        random_state=100,\n",
    "                                                        chunksize=100,\n",
    "                                                        passes=50,\n",
    "                                                        workers=20,\n",
    "                                                        iterations=150,\n",
    "                                                        minimum_probability=0)\n",
    "                                                                 \n",
    "\n",
    "    #here we are going to calculate a coherence metric (c_v coherence, which has been shown to be most close to what a human judges as a coherent split)\n",
    "    #in order to ascertain whether a certain topic split gives rise to coherent topics\n",
    "    #this is a mathematical metric that quantifies how words co-occur together in a given topic to ascertain how coherent that topic is\n",
    "    cv_coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    cv_coherence_total = cv_coherence_model_lda.get_coherence()\n",
    "            \n",
    "    #here we are going to append our resuls to the dataframe we created initially  \n",
    "    results['Topics'].append(i)\n",
    "    results['cv_Coherence_avg'].append(cv_coherence_total)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Topics', ylabel='cv_Coherence_avg'>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this graph will illustrate where the highest coherence is. the peak represents the optimal number of topics for this data.\n",
    "#mind that this data is neither complete, nor is the LDA function calibrated for you to identify the peak coherence easily.\n",
    "#these processes take time and trial and error. \n",
    "#have a play with different chunksizes, passes and iterations - can you now run the function and identify a peak?\n",
    "\n",
    "sns.lineplot(x='Topics', y='cv_Coherence_avg', data=pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topics': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 'cv_Coherence_avg': [0.32623985326377275,\n",
       "  0.3487294019255829,\n",
       "  0.3981024878002298,\n",
       "  0.4282947378030847,\n",
       "  0.38309929673966303,\n",
       "  0.4136473777378204,\n",
       "  0.41534542288343224,\n",
       "  0.40740255628612054,\n",
       "  0.42643649540247636]}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['boredom', 'nothing', 'drug', 'else', 'go', 'use', 'pandemic', 'using', 'something', 'month']\n",
      "Topic: 1 \n",
      "Words: ['time', 'smoking', 'cannabis', 'weed', 'le', 'bored', 'smoke', 'try', 'boredom', 'used']\n",
      "Topic: 2 \n",
      "Words: ['boredom', 'home', 'help', 'sleep', 'anxiety', 'cannabis', 'use', 'lockdown', 'stress', 'relax']\n",
      "Topic: 3 \n",
      "Words: ['cannabis', 'live', 'stress', 'make', 'need', 'fun', 'taking', 'much', 'friend', 'able']\n"
     ]
    }
   ],
   "source": [
    "#run the algorithm for the optimal amount of topics \n",
    "#use the top words to identify your topics\n",
    "\n",
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=4, \n",
    "                                                    random_state=100,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=50,\n",
    "                                                    workers=20,\n",
    "                                                    iterations=150,\n",
    "                                                    minimum_probability=0)\n",
    "\n",
    "#print each topic and its top 10 words to identify what your topic is about!\n",
    "#change the number of words printed by giving a different value to num_words. is the topic easier to interpret now?\n",
    "\n",
    "for idx, topic in lda_model.show_topics(formatted=False, num_words= 10):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the topic probability matrix\n",
    "#this will indicate where each of the reasons in the data belongs to a topic\n",
    "\n",
    "#get topic distribution probabilities\n",
    "all_topics = lda_model.get_document_topics(corpus, minimum_probability=0.0)\n",
    "doc_topic_dist_proc = gensim.matutils.corpus2csc(all_topics)\n",
    "doc_topic_dist_numpy = doc_topic_dist_proc.T.toarray()\n",
    "#doc_topic_dist = pd.DataFrame(doc_topic_dist_numpy)\n",
    "    \n",
    "#get dominant topic\n",
    "topicnames = ['Topic_' + str(i+1) for i in range(0,4,1)] #7 is the number of topics!!\n",
    "all_topics_df = pd.DataFrame(doc_topic_dist_numpy,columns=topicnames)\n",
    "all_topics_df['dominant_topic_contribution'] = all_topics_df.max(axis = 1) \n",
    "all_topics_df['dominant_topic'] = np.argmax(all_topics_df.values, axis=1)\n",
    "all_topics_df['dominant_topic_name'] = \"Topic \"+(all_topics_df['dominant_topic']+1).astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>dominant_topic_contribution</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>dominant_topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic_1  Topic_2  Topic_3  Topic_4  dominant_topic_contribution  \\\n",
       "0       0.04     0.04     0.89     0.04                         0.89   \n",
       "1       0.75     0.08     0.08     0.08                         0.75   \n",
       "2       0.13     0.13     0.62     0.13                         0.62   \n",
       "3       0.02     0.93     0.02     0.02                         0.93   \n",
       "4       0.09     0.75     0.08     0.08                         0.75   \n",
       "..       ...      ...      ...      ...                          ...   \n",
       "276     0.13     0.13     0.62     0.13                         0.62   \n",
       "277     0.13     0.13     0.62     0.13                         0.62   \n",
       "278     0.08     0.41     0.43     0.08                         0.43   \n",
       "279     0.05     0.85     0.05     0.05                         0.85   \n",
       "280     0.06     0.56     0.32     0.06                         0.56   \n",
       "\n",
       "     dominant_topic dominant_topic_name  \n",
       "0                 2             Topic 3  \n",
       "1                 0             Topic 1  \n",
       "2                 2             Topic 3  \n",
       "3                 1             Topic 2  \n",
       "4                 1             Topic 2  \n",
       "..              ...                 ...  \n",
       "276               2             Topic 3  \n",
       "277               2             Topic 3  \n",
       "278               2             Topic 3  \n",
       "279               1             Topic 2  \n",
       "280               1             Topic 2  \n",
       "\n",
       "[281 rows x 7 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each of the columns illustrates the probability of that reason being part of that topic \n",
    "\n",
    "all_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic 3    98\n",
       "Topic 2    92\n",
       "Topic 1    54\n",
       "Topic 4    37\n",
       "Name: dominant_topic_name, dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the distribution of topics in your data\n",
    "\n",
    "all_topics_df.dominant_topic_name.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
