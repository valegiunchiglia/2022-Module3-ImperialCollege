{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                               Workshop 7: Maria Balaet, Valentina Giunchiglia and Dragos Gruia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling on drug use free text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this workshop is to introduce you the application of **natural language processing (NLP)** algorithms to study free text data. NLP is a branch of artificial intelligence that focuses on trying to understand written and spoken text. In this workshop, we will focus specifically on **topic modelling**, which is an unsupervised machine learning method that automatically analyses groups of words to identify clusters of words that belong to the same topic, or theme.\n",
    "\n",
    "In particular, we will use topic. modelling to try to understand the reasons behind the change in drug use patterns of recreational drug users during the early stages of the pandemic. In the morning, we will investigate why recreational drug users decided to increase their use, later in the day you will apply what you learnt in this stage of the workshop to understand why they decided to decrease their drug use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download and import the packages we will need during the lecture, and to change the display settings in order to be able to visualise more rows and columns when printing dataframes. Today, we will work a lot with two new python modules called `gensim` and `nltk`. `gensim` is one of the most coimmonly used module for topic modelling in Python, and `nltk` is a NLP Python toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings \n",
    "import gensim\n",
    "import nltk\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "sb.set_theme(\"talk\")\n",
    "sb.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the data that we will use during the workshop and let's check them out. As you can see, the dataframe consists of three columns that report the user ids, whether the drug use of participants increased or decreased during the pandemic and the reasons behind this change in format of free text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more = pd.read_csv(\"Data/Day7_morning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>How has your drug use changed due to the pandemic?</th>\n",
       "      <th>Why has your drug use changed during the pandemic?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107cee6d-0024-4861-90be-2f467758776c</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Increase in marijuana use for relaxation/stress relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2feae3b8-ea3b-4750-acec-02418f995199</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Microdosing psilocybin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a476ba2-6dbb-42d1-b0f5-c877c0f368d8</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3f8b0ac6-200f-49b4-b061-70084536d519</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4be44530-aeab-4302-bd92-b000b4eae2fb</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Went to university</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>6fbf9bf3-1e5a-4fde-98be-6015accde42b</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>e262433c-094a-4f35-8b6a-0c6689b2185f</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Boredom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>8862877f-c12e-4680-8b7f-04657ae97cbd</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>More time at home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>035a1239-932f-4dc2-9ae8-4ccad6e7be08</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>I'm more bored and frightened when I read the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>b277b233-b4b3-4fe0-9b5c-ec6cd26a67d8</td>\n",
       "      <td>I am using more</td>\n",
       "      <td>Spending more time at home.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  user_id  \\\n",
       "0    107cee6d-0024-4861-90be-2f467758776c   \n",
       "1    2feae3b8-ea3b-4750-acec-02418f995199   \n",
       "2    3a476ba2-6dbb-42d1-b0f5-c877c0f368d8   \n",
       "3    3f8b0ac6-200f-49b4-b061-70084536d519   \n",
       "4    4be44530-aeab-4302-bd92-b000b4eae2fb   \n",
       "..                                    ...   \n",
       "285  6fbf9bf3-1e5a-4fde-98be-6015accde42b   \n",
       "286  e262433c-094a-4f35-8b6a-0c6689b2185f   \n",
       "287  8862877f-c12e-4680-8b7f-04657ae97cbd   \n",
       "288  035a1239-932f-4dc2-9ae8-4ccad6e7be08   \n",
       "289  b277b233-b4b3-4fe0-9b5c-ec6cd26a67d8   \n",
       "\n",
       "    How has your drug use changed due to the pandemic?  \\\n",
       "0                                      I am using more   \n",
       "1                                      I am using more   \n",
       "2                                      I am using more   \n",
       "3                                      I am using more   \n",
       "4                                      I am using more   \n",
       "..                                                 ...   \n",
       "285                                    I am using more   \n",
       "286                                    I am using more   \n",
       "287                                    I am using more   \n",
       "288                                    I am using more   \n",
       "289                                    I am using more   \n",
       "\n",
       "                                                                         Why has your drug use changed during the pandemic?  \n",
       "0                                                                    Increase in marijuana use for relaxation/stress relief  \n",
       "1                                                                                                   Microdosing psilocybin   \n",
       "2                                                                                                                   Boredom  \n",
       "3    I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic  \n",
       "4                                                                                                       Went to university   \n",
       "..                                                                                                                      ...  \n",
       "285                                                                                                                 boredom  \n",
       "286                                                                                                                Boredom   \n",
       "287                                                                                                       More time at home  \n",
       "288                                                                      I'm more bored and frightened when I read the news  \n",
       "289                                                                                            Spending more time at home.   \n",
       "\n",
       "[290 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasons_for_change_dec2020_more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------\n",
    "### Code here\n",
    "The column names of the dataframes are quite long and the row names are not really easy to interpret. \n",
    "\n",
    "1. Replace the column headers with the names \"how\" and \"why\" respectively for \"How has your drug use changed due to the pandemic?\" and \"Why has your drug use changed during the pandemic?\". \n",
    "2. Set the user ids as row index\n",
    "3. Since we are analysing only the participants, the \"how\" column should have only \"I am using more\" answers, check that it is the case.\n",
    "4. Confirm that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "Now that we have a better looking dataframe, let's check out the answers in the *why* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By quickly looking at the answers, it appears that one of the main reasons for starting to use more drugs during the pandemic was *boredom*. However, different people express the same concept in slightly different ways. Let's print a few answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m bored and have more free time. Also I have found more people that sell shrooms, which is unrelated to the pandemic \n",
      " Boredom/accessibility/situational. I have done E twice during this period, separated by around 3 months, and ket twice, separated by about 4 months, so I'm not concerned about restricting myself, or doing so to excess. \n",
      " Boredom and loneliness \n"
     ]
    }
   ],
   "source": [
    "print(reasons_for_change_dec2020_more['why'][3], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][14],\"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][34]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look more carefully, you will notice a few other things: 1) some people wrote boredom with or without capital letters, 2) some answers have empty spaces, 3) other answers have special characters, 4) some words are spelled incorrectly... All these aspects are just few examples of the noise that free text answers have, and that need to be removed before starting any analysis. The data cleaning step in free text analysis is **fundamental**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boredom \n",
      " Boredom  \n",
      " / \n",
      " Boredom but stopped.now \n",
      " Bordem \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reasons_for_change_dec2020_more['why'][285], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][286],\"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][281], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][278], \"\\n\",\n",
    "      reasons_for_change_dec2020_more['why'][273], \"\\n\",\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning of free text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data cleaning is necessary to remove errors in the data, and reduce to the minimum the noise in order to include in the analysis only what is essential. The most important data cleaning steps of free text data are the following:\n",
    "\n",
    "1. **Turning all letters to lower case:** this is important otherwise words with capital letters will be mistakenly be recognised as different compared to the same words without capital letters (e.g This and this).\n",
    "2. **Removal of punctuation, special characters and digits**: punctuation creates noise in the data. It cannot be used to make sense of the meaning of a topic because it does not represent words and computer don't know how to interpret it.\n",
    "3. **Tokenization**: method that consists of separating a piece of text (in this case the answers of each participants) into smaller linguistic units called tokens (in this case words). \n",
    "4. **Stop words removal**: method that consists in removing words that are really common in english, but don't provide much information, such as \"to\", \"in\" or \"when. These words just create noise in the data.\n",
    "5. **Lemmatization**: process of \n",
    "6. **Removal of empty answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete these cleaning steps, in order to understand better what they do and why they are important. First, we will turn everything to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove all punctuations (e.g. *,* or *.*), special characters (e.g. *?,/* and *&*) and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.replace('[,\\.!?/&]', '')\n",
    "reasons_for_change_dec2020_more['why'] = reasons_for_change_dec2020_more['why'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the free text data into a separate variable, and let's check it out. Were all digits, punctuations, and special characters removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reasons_for_change_dec2020_more['why'].to_list()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of the data cleaning and preparation is **tokenization**. Tokenization is necessary to make the sentences analysable and understandable for the computer, and consists of splitting each answer of participants into lists of individual words. There is a function in the gensim package that can do this directly, by taking as input each separate answer, called `simple_preprocess`. By providing as argument `deacc=True`, the function removes punctuations if it finds any. In addition, the function removes all words that have less that 2 letters or more than 20. Words with less than 2 letters are usually not really meaningfull, and those with more than 20 are oddly long! They could be just some typing mistakes.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fewer', 'events', 'makes', 'for', 'more', 'exciting', 'evening']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = []\n",
    "for sentence in data:\n",
    "    listwords = gensim.utils.simple_preprocess(str(sentence), deacc=True, min_len=2, max_len=20)\n",
    "    data_words.append(listwords)\n",
    "    \n",
    "data_words[10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check different elements in the *data_words* list. Do yo understand how tokenization works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the answers in terms of lists of words, we can do some cleaning on the words themselves. The first thing we are going to do is to remove the stop words, or commonly used words in the english language. Luckily, the *nltk* module has alredy a list of these words that we can simply download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valentinagiunchiglia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = nltk.corpus.stopwords.words('english') \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if you think that other words are too common and should be removed, but they are not in this list, you can easily add them. Adding extra filter is always a good idea. In this way, the noise in the data is reduced even more. Do you have any other words in mind? If you do, add it to the following list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'goes', 'with']\n"
     ]
    }
   ],
   "source": [
    "custom_stop = ['goes','with'] \n",
    "finalstop = stop + custom_stop\n",
    "print(finalstop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final list of stop words, we can remove those words from the list of words of each participants' answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['fewer', 'events', 'makes', 'for', 'more', 'exciting', 'evening']\n",
      "After: ['fewer', 'events', 'makes', 'exciting', 'evening']\n"
     ]
    }
   ],
   "source": [
    "data_words_final = []\n",
    "\n",
    "for answer in data_words:\n",
    "    data_words_cleaned = []\n",
    "    for word in answer:\n",
    "        if word not in finalstop:\n",
    "            data_words_cleaned.append(word)\n",
    "    data_words_final.append(data_words_cleaned)\n",
    "\n",
    "print(\"Before:\" , data_words[10]  ) \n",
    "print(\"After:\" , data_words_final[10]  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stop words \"for\" and \"more\" were removed.\n",
    "\n",
    "Now, we can complete the final step of the pre-processing, which is the **lemmatization** step. Lemmatization groups together different inflected forms of words, so that they can be analysed as a unique item. For example, it converts the different conjugations of verbs into the inifinite forms (e.g. \"swim\", \"swam\" and \"swum\" would be all converted to \"swim\"), or turns the plurals of nouns into the singular version. \n",
    "\n",
    "In Python, it can be completed with the `WordNetLemmatizer()` method and the `lemmatize`function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['fewer', 'events', 'makes', 'exciting', 'evening']\n",
      "After: ['fewer', 'event', 'make', 'exciting', 'evening']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/valentinagiunchiglia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "data_words_lemmatized = []\n",
    "for answer in data_words_final:\n",
    "    data_words_cleaned = []\n",
    "    for word in answer:\n",
    "        lematized = lemma.lemmatize(word)\n",
    "        data_words_cleaned.append(lematized)\n",
    "    data_words_lemmatized.append(data_words_cleaned)\n",
    "\n",
    "print(\"Before:\" , data_words_final[10]  ) \n",
    "print(\"After:\" , data_words_lemmatized[10]  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \"events\" was converted to \"event\", and \"makes\" to \"make\". Check a few other answers to see what changed.\n",
    "\n",
    "Let's check how many answers we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we have 290 answers, however, after the cleaning, some of these will be completely empty. This usually happens when participants don't reply properly to the questions, and just write down something random. These answers are of course useless and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_lemmatized_cleaned = []\n",
    "for answer in data_words_lemmatized:\n",
    "    if answer:\n",
    "        data_words_lemmatized_cleaned.append(answer)\n",
    "    \n",
    "len(data_words_lemmatized_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the empty answers, we have now 283 fully cleaned answers that can be used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_for_change_dec2020_more_cleaned=data_words_lemmatized_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the workshop of today, we will do topic modelling using **Latent Dirichlet Allocation (LDA)**. LDA is the most popular topic modelling methods, which aims to find the topics within a body of text based on the words it contains. Let's look at the name of the method and try to understand what each word means:\n",
    "\n",
    "- **Latent**: indicates that the model discovers the ‘yet-to-be-found’ or hidden topics from the documents.\n",
    "- **Drichlet**: indicates the two assumptions of LDA - that both the distribution of topics withinin a document and the distribution of words within each topic are Dirichlet distributions (which is a type of probability distribution).\n",
    "- **Allocation** indicates the distribution of topics in the document.\n",
    "\n",
    "LDA assumes that the words within a document can be used determine the topics. LDA assigns each word in a document to different topic, then maps the entire document to a list of topics. Put another way, LDA computes a many-to-many relationship between topics and words, and thus a many-to-many relationship between documents and topics.\n",
    "\n",
    "[Here](https://proceedings.neurips.cc/paper/2001/file/296472c9542ad4d4788d543508116cbc-Paper.pdf) you can find the original paper, if you are really interested to understand the method in details (this is not required)\n",
    "\n",
    "One of the requirements of LDA is to specify the number of topics that should be identified within the set of answers. The number of topics is a user-defined parameter, better called **hyperparamerter**. In machine learning, an hyperparameter is a parameter that is external to the model, that cannot be inferred from the data, and therefore need to be fine-tuned each time depending on the model you are developing and the dataset you are using, in order to find the optimal value.\n",
    "\n",
    "In case of LDA, one of the best approaches to identify the optimal value for the number of topics is to use the **Coherence Score**. The coherence score specifies whether a certain topic split gives rise to coherent topics. The higher is the score, the more coherent are the topics, the better is the selected topic number! Concretely, this score is a mathematical metric that quantifies how words co-occur together in a given topic to ascertain how coherent that topic is. In order to use this score to identify the optimal number of topics, it is necessary to run the LDA analysis with different number of topics and calculate the score for each of them. The number of topics that leads to the highest coherence score will correspond to the optimal number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting the data in the right format in order to be able to run LDA. The LDA function takes as input two main arguments: \n",
    "\n",
    "1. A **dictionary** that has as keys an id number and as values a word (each word is assigned to a different id number)\n",
    "2. The **corpus** which is essentially the list of answers in a bag-of-words format, that corresponds to (word_id, word_count), where the word id corresponds to the id assigned to the word in the dictionary.\n",
    "\n",
    "To better understand what these are, let's create them using the *gensin* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marijuana\n",
      "relaxationstress\n",
      "free\n"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary(reasons_for_change_dec2020_more_cleaned)\n",
    "print(id2word[1])\n",
    "print(id2word[2])\n",
    "print(id2word[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `id2word` is a dictionary, where each key is a number, and the value of that key is a different word. For example, *id* 1 is assigned to the word *marijuana*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer ['increase', 'marijuana', 'use', 'relaxationstress', 'relief']\n",
      "Corpus [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for answer in reasons_for_change_dec2020_more_cleaned:\n",
    "    corpus.append(id2word.doc2bow(answer))\n",
    "\n",
    "print(\"Answer\", reasons_for_change_dec2020_more_cleaned[0])\n",
    "print(\"Corpus\", corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each word of each answer is converted into (word_id, word_count).\n",
    "\n",
    "Now that we have the data in the right format, we can define which numbers of topics we want to test to find what the optimal number is. Today, we will try a maximum of 10 topics. Some data might require many more than 10, but the more topics numbers you test, the more time and computational resources you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 10)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_topics = 1\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "topics_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's run LDA with all the potential numbers of topics!**\n",
    "\n",
    "To be able to do it, we need to complete the following steps:\n",
    "1. Create a results dictionary where we will store the topic number and coherence score at each iteration\n",
    "2. Loop over all the potential number of topics\n",
    "3. Run the LDA model and change the number of topics at each iteration\n",
    "4. Calculate the coherance score for each topic number\n",
    "5. Save the coherence score in the results dictionary\n",
    "\n",
    "Depending on the amount of RAM that your computer has, this step can be more or less slow. To have an overview of how long you still need to wait before the computation is completed, Python has a really nice function and module called `tqdm` that creates progress bars when running for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:22<00:00,  9.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "results = {'Topics_Number': [], \"cv_Coherence_avg\": []}\n",
    "    \n",
    "for n_topics in tqdm(topics_range):\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=n_topics, \n",
    "                                                        random_state=100,\n",
    "                                                        chunksize=100,\n",
    "                                                        passes=50,\n",
    "                                                        workers=20,\n",
    "                                                        iterations=150,\n",
    "                                                        minimum_probability=0)\n",
    "                                                                 \n",
    "    cv_coherence_total = gensim.models.CoherenceModel(model=lda_model, \n",
    "                                            texts=reasons_for_change_dec2020_more_cleaned, \n",
    "                                            dictionary=id2word).get_coherence()\n",
    "            \n",
    "    results['Topics_Number'].append(n_topics)\n",
    "    results['cv_Coherence_avg'].append(cv_coherence_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topics_Number</th>\n",
       "      <th>cv_Coherence_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topics_Number  cv_Coherence_avg\n",
       "0              1              0.35\n",
       "1              2              0.36\n",
       "2              3              0.37\n",
       "3              4              0.44\n",
       "4              5              0.40\n",
       "5              6              0.42\n",
       "6              7              0.42\n",
       "7              8              0.39\n",
       "8              9              0.42"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have the coherance score for each number of topic, we can create a plot to visualize the results and better see where the peak is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Coherence Score')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.lineplot(x='Topics_Number', y='cv_Coherence_avg', data=pd.DataFrame(results))\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot and the results dataframe, the peak is at topic number = 4, which suggests that 4 is the optimal number of topics. However, there is something important to mention here. If you look at the **LdaMulticore** function, there are three arguments that we set for you, which are `passes`, `iterations` and `chunksize`. These three values are also hyperparameters that should be generally fine-tuned. `passes` corresponds to the number of passes through the corpus during training, `chunksize` to the number of documents to be used in each training chunk, and `iterations` to the maximum number of iterations that are completed through the corpus when inferring its topic distribution. Depending on the values set in these three parameters, the results **will change**. \n",
    "\n",
    "Try to change those values and see what happens. \n",
    "\n",
    "In the end, to properly understand what's the best number of topics is to manually look at the final results, and see whether the identified topics make sense using a more qualitative approach. A combination of a quantitative (e.g. the coherence score) and qualitative approach is usually the best way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we will not fine-tune the number of `passes`, `iterations` and `chunksize`. We already provided to you the best values based on previous reserach experiments. Now that we know that 4 is the optimal topic number, let's run LDA using 4 as number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=4, \n",
    "                                                    random_state=100,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=50,\n",
    "                                                    workers=20,\n",
    "                                                    iterations=150,\n",
    "                                                    minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 4 topics and the top 5 words that represent them using the method `show_topics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('boredom', 0.03706199),\n",
       "   ('cannabis', 0.015576428),\n",
       "   ('weed', 0.015200664),\n",
       "   ('nothing', 0.015123372),\n",
       "   ('lockdown', 0.011541362)]),\n",
       " (1,\n",
       "  [('boredom', 0.070597),\n",
       "   ('bored', 0.024254527),\n",
       "   ('stress', 0.018472504),\n",
       "   ('something', 0.016428465),\n",
       "   ('pandemic', 0.014037038)]),\n",
       " (2,\n",
       "  [('time', 0.060697418),\n",
       "   ('home', 0.027685633),\n",
       "   ('cannabis', 0.020438446),\n",
       "   ('le', 0.016975122),\n",
       "   ('use', 0.015749047)]),\n",
       " (3,\n",
       "  [('cannabis', 0.029808342),\n",
       "   ('help', 0.023188055),\n",
       "   ('relax', 0.019601343),\n",
       "   ('sleep', 0.019273259),\n",
       "   ('smoking', 0.017234659)])]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(formatted=False, num_words= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output of the function is the topic number, and a list of (word, probability) for each topic, where the probability corresponds to the probability of each word for that specific topic. Let's print only the words and ignore the probabilities for a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['boredom', 'cannabis', 'weed', 'nothing', 'lockdown', 'else', 'time', 'drug', 'thing', 'due']\n",
      "Topic: 1 \n",
      "Words: ['boredom', 'bored', 'stress', 'something', 'pandemic', 'university', 'drug', 'use', 'cannabis', 'friend']\n",
      "Topic: 2 \n",
      "Words: ['time', 'home', 'cannabis', 'le', 'use', 'boredom', 'free', 'go', 'smoking', 'lockdown']\n",
      "Topic: 3 \n",
      "Words: ['cannabis', 'help', 'relax', 'sleep', 'smoking', 'evening', 'month', 'smoke', 'thought', 'went']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.show_topics(formatted=False, num_words= 10):\n",
    "    wordskeep = [w[0] for w in topic]\n",
    "    print(f'Topic: {idx} \\nWords: {wordskeep}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe each topic in one sentence? If you struggle to do it, try to increase the number of words that you print for each topic. Is the topic easier to understand now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use the output of LDA to identify what it the dominant topic in each one of the answers of the participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the topic probability matrix\n",
    "#this will indicate where each of the reasons in the data belongs to a topic\n",
    "\n",
    "#get topic distribution probabilities\n",
    "all_topics = lda_model.get_document_topics(corpus, minimum_probability=0.0)\n",
    "doc_topic_dist_proc = gensim.matutils.corpus2csc(all_topics)\n",
    "doc_topic_dist_numpy = doc_topic_dist_proc.T.toarray()\n",
    "#doc_topic_dist = pd.DataFrame(doc_topic_dist_numpy)\n",
    "    \n",
    "#get dominant topic\n",
    "topicnames = ['Topic_' + str(i+1) for i in range(0,4,1)] #7 is the number of topics!!\n",
    "all_topics_df = pd.DataFrame(doc_topic_dist_numpy,columns=topicnames)\n",
    "all_topics_df['dominant_topic_contribution'] = all_topics_df.max(axis = 1) \n",
    "all_topics_df['dominant_topic'] = np.argmax(all_topics_df.values, axis=1)\n",
    "all_topics_df['dominant_topic_name'] = \"Topic \"+(all_topics_df['dominant_topic']+1).astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>dominant_topic_contribution</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>dominant_topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>Topic 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "      <td>Topic 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic_1  Topic_2  Topic_3  Topic_4  dominant_topic_contribution  \\\n",
       "0       0.04     0.04     0.89     0.04                         0.89   \n",
       "1       0.75     0.08     0.08     0.08                         0.75   \n",
       "2       0.13     0.13     0.62     0.13                         0.62   \n",
       "3       0.02     0.93     0.02     0.02                         0.93   \n",
       "4       0.09     0.75     0.08     0.08                         0.75   \n",
       "..       ...      ...      ...      ...                          ...   \n",
       "276     0.13     0.13     0.62     0.13                         0.62   \n",
       "277     0.13     0.13     0.62     0.13                         0.62   \n",
       "278     0.08     0.41     0.43     0.08                         0.43   \n",
       "279     0.05     0.85     0.05     0.05                         0.85   \n",
       "280     0.06     0.56     0.32     0.06                         0.56   \n",
       "\n",
       "     dominant_topic dominant_topic_name  \n",
       "0                 2             Topic 3  \n",
       "1                 0             Topic 1  \n",
       "2                 2             Topic 3  \n",
       "3                 1             Topic 2  \n",
       "4                 1             Topic 2  \n",
       "..              ...                 ...  \n",
       "276               2             Topic 3  \n",
       "277               2             Topic 3  \n",
       "278               2             Topic 3  \n",
       "279               1             Topic 2  \n",
       "280               1             Topic 2  \n",
       "\n",
       "[281 rows x 7 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each of the columns illustrates the probability of that reason being part of that topic \n",
    "\n",
    "all_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic 3    98\n",
       "Topic 2    92\n",
       "Topic 1    54\n",
       "Topic 4    37\n",
       "Name: dominant_topic_name, dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at the distribution of topics in your data\n",
    "\n",
    "all_topics_df.dominant_topic_name.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
